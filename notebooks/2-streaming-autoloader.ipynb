{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5afb23db-e89f-4105-b75c-7c6dcfd76cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pytest==8.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45ddf3f4-ece4-447c-b897-d8658b74ad98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âš ï¸ Important Notice: Use a Personal Cluster for Structured Streaming\n",
    "\n",
    "**Attention students:**  \n",
    "\n",
    "This notebook uses **Structured Streaming** features that are **not fully supported in serverless clusters**.  \n",
    "\n",
    "To ensure full functionality, including:   \n",
    "- Continuous streaming processing  \n",
    "- Checkpointing and state management  \n",
    "\n",
    "You **must use a personal (interactive) cluster** or a standard Databricks cluster with the correct runtime.  \n",
    "\n",
    "> âŒ Serverless clusters may limit streaming capabilities, prevent proper checkpointing, and can cause runtime errors.  \n",
    "\n",
    "**Recommended actions:**  \n",
    "1. Start a **personal cluster** in your workspace.  \n",
    "2. Make sure the cluster uses a **Databricks Runtime version that supports Structured Streaming** (preferably latest LTS).  \n",
    "3. Attach this notebook to the personal cluster before running or creating your DLT pipeline.  \n",
    "\n",
    "Following this ensures your streaming pipelines work correctly and all transformations execute as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ee0f79-eb5b-4700-b210-692d22160f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "from helpers import utils, generate_streaming_events as gen_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ee08a0f-d4b8-4dd0-aa1d-992ceff9dfd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ—‚ï¸ Defining Paths and Environment Variables\n",
    "\n",
    "In this step, we will **set up the environment and directory structure** that will be used during the streaming process.\n",
    "\n",
    "- `capstone_env`: defines the environment (e.g., `dev`, `prod`), making the code reusable across different stages.  \n",
    "- `base_schema`: retrieves the base schema name using the helper function `utils.get_base_user_schema()`.  \n",
    "- `volume_base_path`: base directory path where all data and metadata related to the stream will be stored.  \n",
    "- `raw_path`: location where the raw event files will be written.  \n",
    "- `checkpoint_path`: location where Spark will save checkpoints to keep track of stream progress.  \n",
    "- `schema_location_path`: directory where the evolving schema will be stored.  \n",
    "- `output_table`: name of the Delta table where the processed events will be written.\n",
    "\n",
    "We also print the paths to verify that they were set correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ca1d47-7ad3-40a7-8e70-3edeaef8dd06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from instructors.src.solutions.streaming_ingestion import StreamingIngestion\n",
    "\n",
    "mod2 = StreamingIngestion()\n",
    "catalog = utils.get_param(\"catalog\", \"capstone_dev\")\n",
    "env = utils.get_param(\"env\", \"dev\")\n",
    "bronze_schema = f\"{utils.get_base_user_schema()}_bronze\"\n",
    "\n",
    "volume_base_path = f\"/Volumes/{catalog}/{bronze_schema}\"\n",
    "raw_path = f\"{volume_base_path}/raw_files/web_site_events\"\n",
    "checkpoint_path = f\"{volume_base_path}/checkpoint_files/web_site_events\"\n",
    "schema_location_path = f\"{volume_base_path}/schema_files/web_site_events\"\n",
    "output_table = f\"{catalog}.{bronze_schema}.web_site_events\"\n",
    "\n",
    "print(f\"RAW PATH: {raw_path}\")\n",
    "print(f\"CHECKPOINT: {checkpoint_path}\")\n",
    "print(f\"SCHEMA LOCATION: {schema_location_path}\")\n",
    "print(f\"TABLE: {output_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82d4d52c-0858-4ecc-8870-bac991136592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ“¥ Defining a Function to Read Streaming Data (JSON)\n",
    "\n",
    "In this step, we define a helper function called `read_stream_json` to **read JSON files in streaming mode** using **Auto Loader**.  \n",
    "\n",
    "This function allows Spark to automatically detect and adapt to schema changes by enabling **schema evolution**.\n",
    "\n",
    "### Function details:\n",
    "- `spark`: the active `SparkSession`.\n",
    "- `source_path`: the path to the directory containing the raw JSON files.\n",
    "- `schema_path`: the path where Spark will store the inferred schema to track changes over time.\n",
    "\n",
    "### Key options:\n",
    "- `cloudFiles.format = \"json\"` â†’ defines the file format.  \n",
    "- `cloudFiles.inferColumnTypes = true` â†’ automatically detects column types.  \n",
    "- `cloudFiles.schemaEvolutionMode = addNewColumns` â†’ allows adding new columns without failing the stream.  \n",
    "- `cloudFiles.schemaLocation` â†’ directory to store schema information.  \n",
    "- `multiLine = true` â†’ supports JSON files spanning multiple lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a02336-8a59-4524-92d8-80c2ff7e823e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_stream_json(self, source_path: str, schema_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    This function returns a streaming DataFrame that can be used for further transformations or writes.\n",
    "    Args:\n",
    "        source_path (str): Path to the directory containing the source JSON files.\n",
    "        schema_path (str): Path to the schema location used by Auto Loader to track and evolve the schema.\n",
    "    Returns:\n",
    "        DataFrame: A streaming DataFrame representing the ingested JSON data.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7148a084-4709-4461-b8ab-db5c053a90bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ’¾ Defining a Function to Write Streaming Data to Delta Lake\n",
    "\n",
    "In this step, we define the helper function `write_stream_to_delta` to **write streaming data to a Delta Lake table** with **schema evolution enabled** through the `mergeSchema` option.\n",
    "\n",
    "### Function details:\n",
    "- `df`: the streaming DataFrame that will be written to Delta Lake.  \n",
    "- `checkpoint_path`: the directory where Spark will store checkpoints to maintain stream state.  \n",
    "- `table_name`: the name of the target Delta table.\n",
    "\n",
    "### Key options:\n",
    "- `format(\"delta\")` â†’ writes the stream in Delta Lake format.  \n",
    "- `checkpointLocation` â†’ stores metadata and progress information for fault tolerance.  \n",
    "- `mergeSchema = true` â†’ allows Delta Lake to automatically update the table schema as new columns appear.  \n",
    "- `outputMode = append` â†’ continuously appends new records to the table.  \n",
    "- `trigger(availableNow=True)` â†’ processes all available data and then stops (useful for development of streaming pipelines).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd2d8339-333d-4f0a-b7f8-51fa279d934b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_stream_to_delta(self, df: DataFrame, checkpoint_path: str, table_name: str) -> StreamingQuery:\n",
    "    \"\"\"\n",
    "    This function returns a `StreamingQuery` object that represents the active streaming job.\n",
    "    Args:\n",
    "        df (DataFrame): The streaming DataFrame to be written to Delta Lake.\n",
    "        checkpoint_path (str): Path to the directory for storing checkpoint data.\n",
    "        table_name (str): Name of the target Delta Lake table.\n",
    "    Returns:\n",
    "        StreamingQuery: The active streaming query writing data to Delta Lake.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ce360de-44ba-49dc-813b-cb98f5a555a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸš€ Running the Streaming Job with the Initial Schema\n",
    "\n",
    "In this step, we will **execute the streaming pipeline using the initial schema** (without schema evolution enabled).\n",
    "\n",
    "### Instructions:\n",
    "1. **Open the simulation script** at: helpers/generate_streaming_events.py\n",
    "\n",
    "2. **Run the script with the parameter**:\n",
    "```bash\n",
    "enable_schema_evolution=False\n",
    "```\n",
    "\n",
    "This ensures that schema evolution is **disabled**, so the stream will not create new columns.\n",
    "\n",
    "*(Optional)* You can adjust the following parameters in the script to test different scenarios:\n",
    "\n",
    "- `batch_size`: number of events per batch\n",
    "- `delay_seconds`: delay between batches\n",
    "- `total_batches`: total number of batches\n",
    "- `null_frequency`: frequency of null values in the data\n",
    "\n",
    "### What this code does:\n",
    "\n",
    "- Retrieves or creates the active `SparkSession`.\n",
    "- Reads streaming JSON data from `raw_path` using the `read_stream_json` function.\n",
    "- Prints the schema inferred from the stream.\n",
    "- Writes the streaming data to a Delta table using `write_stream_to_delta` with checkpointing.\n",
    "- Waits for the streaming query to process all available data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000ae978-eead-4322-a09d-6336a65716f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gen_stream.stream_events(\n",
    "    batch_size=10000,\n",
    "    delay_seconds=1,\n",
    "    total_batches=10,\n",
    "    null_frequency=500,\n",
    "    env=env,\n",
    "    enable_schema_evolution=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ac7a56b-cd51-40b4-8d0f-1f28119431be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_stream = mod2.read_stream_json(raw_path, schema_location_path)\n",
    "df_stream.printSchema()\n",
    "\n",
    "query = mod2.write_stream_to_delta(df_stream, checkpoint_path, output_table)\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9ca71fe-b9e0-4b9a-9525-f1d42a510471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âœ… Validating the Delta Table\n",
    "\n",
    "After running the streaming job, it's important to **verify that the data was correctly written** to the Delta table.\n",
    "\n",
    "The following command:\n",
    "\n",
    "- Executes a simple SQL query on the Delta table defined by `output_table`.  \n",
    "- Retrieves the first 100 records to give a quick preview of the data.  \n",
    "- Displays the results in a tabular format for easy inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "586d4e90-398a-4bb5-b518-d4e9649964b4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760365139383}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM {output_table} limit 100\").display()\n",
    "spark.sql(f\"SELECT COUNT(*) FROM {output_table}\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2997a13-56fa-435f-b72f-c562f578b296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## ðŸš€ Running the Streaming Job with Schema Evolution Enabled\n",
    "\n",
    "In this step, we will **execute the streaming pipeline with schema evolution enabled**.  \n",
    "This allows Spark Structured Streaming to **automatically detect and incorporate new columns** into the Delta table schema as they appear in the incoming data.\n",
    "\n",
    "### Instructions:\n",
    "1. **Open the simulation script** at: `helpers/generate_streaming_events.py`\n",
    "\n",
    "2. **Run the script with the parameter**:\n",
    "```bash\n",
    "enable_schema_evolution=True\n",
    "```\n",
    "This enables Spark to **adapt to schema changes** during the streaming process.\n",
    "\n",
    "*(Optional)* You can adjust the same simulation parameters as before to test different scenarios:\n",
    "\n",
    "- `batch_size`: number of events per batch\n",
    "- `delay_seconds`: delay between batches\n",
    "- `total_batches`: total number of batches\n",
    "- `null_frequency`: frequency of null values in the data\n",
    "\n",
    "### What this code does:\n",
    "\n",
    "- Retrieves or creates the active `SparkSession`.\n",
    "- Reads streaming JSON data from `raw_path` using the `read_stream_json` function with **schema evolution support**.\n",
    "- Prints the schema inferred from the stream, showing any new columns added.\n",
    "- Writes the streaming data to a Delta table using `write_stream_to_delta` with **mergeSchema enabled**.\n",
    "- Waits for the streaming query to process all available data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7537bc49-b093-43b1-9f00-188305cd348c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gen_stream.stream_events(\n",
    "    batch_size=10000,\n",
    "    delay_seconds=1,\n",
    "    total_batches=10,\n",
    "    null_frequency=500,\n",
    "    env=env,\n",
    "    enable_schema_evolution=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02841b6e-d546-44a2-8e06-d9154496c2b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_stream = mod2.read_stream_json(raw_path, schema_location_path)\n",
    "df_stream.printSchema()\n",
    "\n",
    "query = mod2.write_stream_to_delta(df_stream, checkpoint_path, output_table)\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "840c461f-aeb6-426a-8228-1cb47e8b5ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Did the read/write fail?\n",
    "> **Note:**  \n",
    "> Remember the default behavior for autoloader in mode `addNewColumns` is:\n",
    ">\n",
    "> - Stream fails. \n",
    "> - New columns are added to the schema. \n",
    "> - Existing columns do not evolve data types.  \n",
    ">\n",
    "> We need to **rerun** the read/write operation to see the changes reflected on the destination table.  \n",
    ">\n",
    "> Checking at the schema folder / volume, you should be able to see two different versions.  \n",
    ">\n",
    "> See more: https://docs.databricks.com/aws/en/ingestion/cloud-object-storage/auto-loader/schema#how-does-auto-loader-schema-evolution-work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4080d4a-3fb9-4c44-8e2e-8c7bf0955859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âœ… Validating the Delta Table After Schema Evolution\n",
    "\n",
    "After running the streaming job with **schema evolution enabled**, it's important to **verify that new columns were correctly added** to the Delta table.\n",
    "\n",
    "The following command:\n",
    "\n",
    "- Executes a SQL query on the Delta table defined by `output_table`.  \n",
    "- Filters for records where the newly added column `referrer_url` is not null, helping to confirm that schema evolution worked.  \n",
    "- Retrieves the first 20 records to quickly inspect the updated schema and data.  \n",
    "- Displays the results in a tabular format for easy observation of the new column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c136a6d-aac4-45ba-8751-918cbb2fa5b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM {output_table} where referrer_url is not null limit 20\").display()\n",
    "spark.sql(f\"SELECT COUNT(*) FROM {output_table}\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cd18d28-3bdb-4109-b6c9-d5d0abfd7fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tests\n",
    "**TO DO:**  \n",
    "- Check if there are any failed tests and investigate their root cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b92070f-ff00-4c7a-a307-701668213cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from helpers import test_runner\n",
    "import os\n",
    "\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "os.environ[\"NOTEBOOK_NAME\"] = notebook_path.split(\"/\")[-1]\n",
    "\n",
    "test_runner.run(\n",
    "    df=spark.sql(f\"SELECT * FROM {output_table}\"),\n",
    "    schema_evoluted=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7355612748268555,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2-streaming-autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
