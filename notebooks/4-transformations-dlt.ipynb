{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4b09027-5fca-49ca-8fa9-8e463a1c5c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pytest==8.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27bfd1e4-f146-4f98-8502-3a431df83052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta Live Tables - Stream-Static Join Example\n",
    "\n",
    "In this notebook you need to build a Delta Live Tables pipeline that:\n",
    "- Reads `web_site_events` from an existing Bronze table (streaming source).\n",
    "- Reads `product` and `user` lookup tables from Silver (static sources).\n",
    "- Cleans the data by removing null `user_id` values through an `INNER JOIN`.\n",
    "- Enriches the events with product and user information.\n",
    "- Outputs a Silver layer table.\n",
    "\n",
    "We use:\n",
    "- `dlt.read_stream` for streaming ingestion.\n",
    "- `dlt.read` for batch/static lookups.\n",
    "- `@dlt.table` to declare the DLT tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cfd499d-ed8e-4b4c-a209-ccb9cb2a9742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> **Key Note:**\n",
    "> During the notebook you will find a series of function definitions. They're only a guide, so feel free to use or remove them, and create your own pieces of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "770d694a-062e-4fe5-b063-7074da8acfc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver: Stream-Static Join\n",
    "\n",
    "Create the **Silver layer** by joining streaming events with static lookup tables:\n",
    "\n",
    "- `INNER JOIN` with `user` removes any event records where `user_id` is null or does not match.\n",
    "- `LEFT JOIN` with `product` enriches events with product information.\n",
    "- The result is a clean, enriched Silver table.\n",
    "\n",
    "**Important:**  \n",
    "- Streaming source: `web_site_events_bronze`  \n",
    "- Static sources: `user_bronze`, `product_bronze`\n",
    "\n",
    "## Expect output schema\n",
    "- event_id\n",
    "- event_timestamp\n",
    "- event_type\n",
    "- session_id\n",
    "- device_type\n",
    "- endpoint\n",
    "- referrer_url\n",
    "- user_id\n",
    "- user_name\n",
    "- user_email\n",
    "- user_phone\n",
    "- is_active_user\n",
    "- product_id\n",
    "- product_name\n",
    "- product_category\n",
    "- product_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fd410e6-6137-487a-9473-eb9f05e23120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "# ðŸ§° Get configuration from pipeline parameters\n",
    "catalog = spark.conf.get(\"catalog\")\n",
    "bronze_schema = spark.conf.get(\"bronze_schema\")\n",
    "silver_schema = spark.conf.get(\"silver_schema\")\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"web_site_events\",\n",
    "    comment=\"Silver layer - enriched web site events after stream-static join.\"\n",
    ")\n",
    "def web_site_events_silver():\n",
    "    \"\"\"\n",
    "    Creates the Silver layer Delta Live Table (DLT) for enriched website events.\n",
    "\n",
    "    This function reads streaming website event data from the Bronze layer and \n",
    "    enriches it with static user and product dimension data from the Silver layer. \n",
    "    It performs the following transformations:\n",
    "      - Reads the `web_site_events` stream from the Bronze schema.\n",
    "      - Reads and cleans the `products` and `users` tables from the Silver schema.\n",
    "      - Joins user and product data to the events stream.\n",
    "      - Selects and renames key fields for the Silver layer.\n",
    "\n",
    "    The resulting table provides a curated, query-ready dataset combining event,\n",
    "    user, and product information for downstream analytics.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A transformed and enriched Spark DataFrame containing:\n",
    "            - event_id\n",
    "            - event_timestamp\n",
    "            - event_type\n",
    "            - session_id\n",
    "            - device_type\n",
    "            - endpoint\n",
    "            - referrer_url\n",
    "            - user_id\n",
    "            - user_name\n",
    "            - user_email\n",
    "            - user_phone\n",
    "            - is_active_user\n",
    "            - product_id\n",
    "            - product_name\n",
    "            - product_category\n",
    "            - product_price\n",
    "    \"\"\"\n",
    "\n",
    "    events_stream = dlt.read_stream(f\"{catalog}.{bronze_schema}.web_site_events\")\n",
    "\n",
    "    products = dlt.read(f\"{catalog}.{silver_schema}.products\") \\\n",
    "        .drop(\"last_modified\") \\\n",
    "        .withColumnRenamed(\"name\", \"product_name\")\n",
    "\n",
    "    users = dlt.read(f\"{catalog}.{silver_schema}.users\")\\\n",
    "        .drop(\"last_modified\") \\\n",
    "        .withColumnRenamed(\"name\", \"user_name\")\n",
    "\n",
    "    return (\n",
    "        events_stream\n",
    "            .join(users, on=\"user_id\", how=\"inner\")\n",
    "            .join(products, on=\"product_id\", how=\"left\")\n",
    "            .selectExpr(\n",
    "                \"event_id\",\n",
    "                \"event_timestamp\",\n",
    "                \"event_type\",\n",
    "                \"session_id\",\n",
    "                \"device_type\",\n",
    "                \"endpoint\",\n",
    "                \"referrer_url\",\n",
    "                \"user_id\",\n",
    "                \"user_name\",\n",
    "                \"email as user_email\",\n",
    "                \"phone as user_phone\",\n",
    "                \"is_active as is_active_user\",\n",
    "                \"product_id\",\n",
    "                \"product_name\",\n",
    "                \"category as product_category\",\n",
    "                \"price as product_price\"\n",
    "            )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85c449f1-3f3a-4b0b-aba6-338bfb0c7014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# How to Run the DLT Notebook in Databricks\n",
    "\n",
    "To execute this Delta Live Tables (DLT) notebook:\n",
    "\n",
    "1. Go to **Jobs & Pipelines** â†’ **ETL Pipeline**.\n",
    "2. **Disable the Lakehouse Flow editor** option.\n",
    "3. Give your pipeline a **name**.\n",
    "4. Select **Serverless** for the cluster type.\n",
    "5. Set **Pipeline Mode** to **Continuous** for production and **Triggered** for development and test.\n",
    "6. In **Source Code**, select this notebook you just created.\n",
    "7. Choose the desired **catalog** (dev or prod) and the **schema** for the Silver layer.\n",
    "8. Create the 3 required configurations (catalog, bronze_schema, silver_schema)\n",
    "9. Click **Create** to finish setup.\n",
    "\n",
    "The pipeline will now run continuously, processing incoming streaming data and creating the Silver table `web_site_events`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15ea1304-4c6e-474e-bc61-f97d791414ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Tests for Delta Live Tables Pipeline\n",
    "\n",
    "The following cells contain tests to validate the correctness of the Delta Live Tables pipeline defined above. These tests ensure that the pipeline behaves as expected under various scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17bb49c2-0cb3-4cef-aafc-00f6fb1c765e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from helpers import test_runner\n",
    "import os\n",
    "\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "os.environ[\"NOTEBOOK_NAME\"] = notebook_path.split(\"/\")[-1]\n",
    "\n",
    "test_runner.run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7355612748268557,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4-transformations-dlt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
